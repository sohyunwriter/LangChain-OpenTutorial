{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangSmith Online Evaluation\n",
    "\n",
    "- Author: [JeongGi Park](https://github.com/jeongkpa)\n",
    "- Design: []()\n",
    "- Peer Review: \n",
    "- This is a part of [LangChain Open Tutorial](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial)\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/01-Basic/07-LCEL-Interface.ipynb) [![Open in GitHub](https://img.shields.io/badge/Open%20in%20GitHub-181717?style=flat-square&logo=github&logoColor=white)](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/01-Basic/07-LCEL-Interface.ipynb)\n",
    "\n",
    "## Overview\n",
    "This notebook provides tools to evaluate and track the performance of language models using **LangSmith's** online evaluation capabilities.\n",
    "\n",
    "\n",
    "By setting up chains and using custom configurations, users can assess model outputs, including **hallucination** detection and context recall, ensuring robust performance in various scenarios.\n",
    "\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "- [Overview](#overview)\n",
    "- [Environment Setup](#environment-setup)\n",
    "- [Build a Pipeline for Online Evaluations](#Build-a-Pipeline-for-Online-Evaluations)\n",
    "- [Set Up the RAG System with PDFRAG](#Set-Up-the-RAG-System-with-PDFRAG)\n",
    "- [Create a Parallel Evaluation Runnable](#Create-a-Parallel-Evaluation-Runnable)\n",
    "- [Make Online LLM-as-judge](#Make-Online-LLM-as-judge)\n",
    "- [Run Evaluations](#Run-Evaluations)\n",
    "\n",
    "### References\n",
    "\n",
    "- [Langsmith DOC](https://docs.smith.langchain.com/)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Set up the environment. You may refer to [Environment Setup](https://wikidocs.net/257836) for more details.\n",
    "\n",
    "**[Note]**\n",
    "- `langchain-opentutorial` is a package that provides a set of easy-to-use environment setup, useful functions and utilities for tutorials. \n",
    "- You can checkout the [`langchain-opentutorial`](https://github.com/LangChain-OpenTutorial/langchain-opentutorial-pypi) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install langchain-opentutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "from langchain_opentutorial import package\n",
    "\n",
    "package.install(\n",
    "    [\n",
    "        \"langsmith\",\n",
    "        \"langchain-openai\",\n",
    "        \"langchain_community\",\n",
    "        \"pymupdf\",\n",
    "        \"faiss-cpu\"\n",
    "    ],\n",
    "    verbose=False,\n",
    "    upgrade=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables have been set successfully.\n"
     ]
    }
   ],
   "source": [
    "# Set environment variables\n",
    "from langchain_opentutorial import set_env\n",
    "\n",
    "set_env(\n",
    "    {\n",
    "        \"OPENAI_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_TRACING_V2\": \"true\",\n",
    "        \"LANGCHAIN_ENDPOINT\": \"https://api.smith.langchain.com\",\n",
    "        \"LANGCHAIN_PROJECT\": \"LangSmith-Online-Evaluation\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Note] If you are using a `.env` file, proceed as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Pipeline for Online Evaluations\n",
    "\n",
    "The provided Python script defines a class `PDFRAG` and related functionality to set up a RAG(Retriever-Augmented Generation) pipeline for online evaluation of language models.\n",
    "\n",
    "### Explain for `PDFRAG`\n",
    "\n",
    "The `PDFRAG` class is a modular framework for:\n",
    "\n",
    "1. Document Loading: Ingesting a PDF document.\n",
    "2. Document Splitting: Dividing the content into manageable chunks for processing.\n",
    "3. Vectorstore Creation: Converting chunks into vector representations using embeddings.\n",
    "4. Retriever Setup: Enabling retrieval of the most relevant chunks for a given query.\n",
    "5. Chain Construction: Creating a QA(Question-Answering) chain with prompt templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "class PDFRAG:\n",
    "    def __init__(self, file_path: str, llm):\n",
    "        self.file_path = file_path\n",
    "        self.llm = llm\n",
    "\n",
    "    def load_documents(self):\n",
    "        # Load Documents\n",
    "        loader = PyMuPDFLoader(self.file_path)\n",
    "        docs = loader.load()\n",
    "        return docs\n",
    "\n",
    "    def split_documents(self, docs):\n",
    "        # Split Documents\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "        split_documents = text_splitter.split_documents(docs)\n",
    "        return split_documents\n",
    "\n",
    "    def create_vectorstore(self, split_documents):\n",
    "        # Embedding\n",
    "        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "        # Create DB\n",
    "        vectorstore = FAISS.from_documents(\n",
    "            documents=split_documents, embedding=embeddings\n",
    "        )\n",
    "        return vectorstore\n",
    "\n",
    "    def create_retriever(self):\n",
    "        vectorstore = self.create_vectorstore(\n",
    "            self.split_documents(self.load_documents())\n",
    "        )\n",
    "        # Retriever\n",
    "        retriever = vectorstore.as_retriever()\n",
    "        return retriever\n",
    "\n",
    "    def create_chain(self, retriever):\n",
    "        # Create Prompt\n",
    "        prompt = PromptTemplate.from_template(\n",
    "            \"\"\"You are an assistant for question-answering tasks. \n",
    "        Use the following pieces of retrieved context to answer the question. \n",
    "        If you don't know the answer, just say that you don't know. \n",
    "\n",
    "        #Context: \n",
    "        {context}\n",
    "\n",
    "        #Question:\n",
    "        {question}\n",
    "\n",
    "        #Answer:\"\"\"\n",
    "        )\n",
    "\n",
    "        # Chain\n",
    "        chain = (\n",
    "            {\n",
    "                \"context\": retriever,\n",
    "                \"question\": RunnablePassthrough(),\n",
    "            }\n",
    "            | prompt\n",
    "            | self.llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "        return chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up the RAG System with PDFRAG\n",
    "\n",
    "The following code demonstrates how to instantiate and use the `PDFRAG` class to set up a RAG(Retriever-Augmented Generation) pipeline using a specific PDF document and a GPT-based model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Create a PDFRAG object\n",
    "rag = PDFRAG(\n",
    "    \"data/Newwhitepaper_Agents2.pdf\",\n",
    "    ChatOpenAI(model=\"gpt-4o-mini\", temperature=0),\n",
    ")\n",
    "\n",
    "# Create a retriever\n",
    "retriever = rag.create_retriever()\n",
    "\n",
    "# Create a chain\n",
    "rag_chain = rag.create_chain(retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Parallel Evaluation Runnable\n",
    "\n",
    "The following code demonstrates how to create a `RunnableParallel` object to evaluate multiple aspects of the RAG(Retriever-Augmented Generation) pipeline concurrently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "# Create a RunnableParallel object.\n",
    "evaluation_runnable = RunnableParallel(\n",
    "    {\n",
    "        \"context\": retriever,\n",
    "        \"answer\": rag_chain,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = evaluation_runnable.invoke(\"How do agents differ from standalone language models?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Online LLM-as-judge\n",
    "\n",
    "This guide explains how to set up and configure an online LLM evaluator using LangSmith. It walks you through creating evaluation rules, configuring API keys and prompts, and targeting specific outputs with tags for precise assessments.\n",
    "\n",
    "\n",
    "### 1. click Add Rule\n",
    "Click “Add Rule” to create a new evaluation rule in your LangSmith project.\n",
    "\n",
    "\n",
    "![make-online-LLM-as-judge](./assets/14-LangSmith-Online-Evaluation-01.png)\n",
    "\n",
    "### 2. Create Evaluator\n",
    "Open the Evaluator creation page to define how your outputs will be judged.\n",
    "\n",
    "\n",
    "![create-evaluator](./assets/14-LangSmith-Online-Evaluation-02.png)\n",
    "\n",
    "\n",
    "### 3. Set Secrets & API Keys\n",
    "Provide the necessary API keys and environment secrets for your LLM provider.\n",
    "\n",
    "\n",
    "![set-secrets-API-keys](./assets/14-LangSmith-Online-Evaluation-03.png)\n",
    "![set-API-Keys](./assets/14-LangSmith-Online-Evaluation-04.png)\n",
    "\n",
    "\n",
    "### 4. Set Provider, Model, Prompt\n",
    "Choose the LLM provider, select a model, and write the prompt you want to use.\n",
    "\n",
    "\n",
    "![set-provider-model-prompt](./assets/14-LangSmith-Online-Evaluation-05.png)\n",
    "\n",
    "\n",
    "### 5. Select Halluciantion\n",
    "Pick the “Hallucination” criteria to evaluate factual accuracy in responses.\n",
    "\n",
    "\n",
    "![select-hallucination](./assets/14-LangSmith-Online-Evaluation-06.png)\n",
    "\n",
    "\n",
    "\n",
    "### 6. Set facts for output.context\n",
    "Enter the factual information in “output.context” so the evaluator can reference it.\n",
    "\n",
    "\n",
    "![set-facts](./assets/14-LangSmith-Online-Evaluation-07.png)\n",
    "\n",
    "### 7. Set answer for output.answer\n",
    "Specify the expected answer in “output.answer” for comparison.\n",
    "\n",
    "\n",
    "![set-answer](./assets/14-LangSmith-Online-Evaluation-08.png)\n",
    "\n",
    "\n",
    "### 8. Check Preview for Data\n",
    "Review your evaluation data in the Preview tab to confirm correctness.\n",
    "\n",
    "\n",
    "![check-preview](./assets/14-LangSmith-Online-Evaluation-09.png)\n",
    "\n",
    "\n",
    "**Caution**\n",
    "\n",
    "You must view the preview and then turn off preview mode again before proceeding to the next step. And you have to fill \"Name\" to continue.\n",
    "\n",
    "![continue](./assets/14-LangSmith-Online-Evaluation-10.png)\n",
    "\n",
    "![fill-evaluator](./assets/14-LangSmith-Online-Evaluation-13.png)\n",
    "\n",
    "### 9. Save and Continue\n",
    "Save your evaluator and click “Continue” to finalize the configuration.\n",
    "\n",
    "\n",
    "![save](./assets/14-LangSmith-Online-Evaluation-11.png)\\\n",
    "![check-for-save](./assets/14-LangSmith-Online-Evaluation-14.png)\n",
    "\n",
    "### 10. Make \"Tag\"\n",
    "Create a tag so you can selectively run evaluations on particular outputs.\n",
    "\n",
    "\n",
    "![make-tag](./assets/14-LangSmith-Online-Evaluation-15.png)\n",
    "\n",
    "Instead of evaluating all steps, you can set \"Tag\" to evaluate only specific tags.\n",
    "\n",
    "\n",
    "### 11. Set \"Tag\" that you want\n",
    "Choose the tag you wish to use for targeted evaluations.\n",
    "\n",
    "\n",
    "![set-tag](./assets/14-LangSmith-Online-Evaluation-16.png)\n",
    "\n",
    "### 12. Run evaluations only for specific tags (hallucination)\n",
    "Trigger the evaluation process exclusively for outputs labeled with your chosen tag.\n",
    "\n",
    "![run-eval](./assets/14-LangSmith-Online-Evaluation-17.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Evaluations\n",
    "\n",
    "The following code demonstrates how to perform evaluations on the RAG(Retriever-Augmented Generation) pipeline, including **hallucination** detection, context recall assessment, and combined evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "# set a tag\n",
    "hallucination_config = RunnableConfig(tags=[\"hallucination_eval\"])\n",
    "context_recall_config = RunnableConfig(tags=[\"context_recall_eval\"])\n",
    "all_eval_config = RunnableConfig(tags=[\"hallucination_eval\", \"context_recall_eval\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run chain\n",
    "_ = evaluation_runnable.invoke(\"How do agents differ from standalone language models?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request a Hallucination evaluation\n",
    "_ = evaluation_runnable.invoke(\n",
    "    \"How do agents differ from standalone language models?\", config=hallucination_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request a Context Recall assessment\n",
    "_ = evaluation_runnable.invoke(\n",
    "    \"How do agents differ from standalone language models?\",\n",
    "    config=context_recall_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All evaluation requests\n",
    "_ = evaluation_runnable.invoke(\n",
    "    \"How do agents differ from standalone language models?\", config=all_eval_config\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-opentutorial-cyV7o7ra-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
